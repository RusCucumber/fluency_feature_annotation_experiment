{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/10 (Fri) | Preprocess\n",
    "\n",
    "# Post-Process for Forced Alignment Outputs of Dialogue Data (ASR Transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook fills star tokens in the forced alignment outputs focusing on dialogue data.\n",
    "The procedure consists of the following stages.\n",
    "\n",
    "1. Get user ids\n",
    "2. Load a transcript text corresponding to a user id\n",
    "3. Get a turn-level ids and transcript\n",
    "4. Load a corresponding forced alignment output data\n",
    "5. Does the forced alignment output data includes star tokens?\n",
    "\n",
    "    a. True: Convert transcript to word list and fill star token\n",
    "        \n",
    "    b. False: Save it with the prefix \"_filled\"\n",
    "\n",
    "Before startint the process, the following code block loads required packages and define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, Generator\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TRANSCRIPT_DIR = Path(\"/home/matsuura/Development/app/feature_extraction_api/experiment/data/WoZ_Interview/02_Rev_Transcript\")\n",
    "FA_DIR = Path(\"/home/matsuura/Development/app/feature_extraction_api/experiment/data/WoZ_Interview/04_FA_csv_Auto\")\n",
    "\n",
    "PUNCTUATIONS = [\".\", \",\", \":\", \"?\", \"!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Define Functions\n",
    "\n",
    "This section defines functions to complete the post-process.\n",
    "The following code block defines a generator of user ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_id_generator() -> Generator[str, None, None]:\n",
    "    for uid in range(1, 86):\n",
    "        uid = str(uid).zfill(3)\n",
    "\n",
    "        yield uid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines functions to load a transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcript(uid: str) -> pd.DataFrame:\n",
    "    csv_path = TRANSCRIPT_DIR / f\"{uid}.csv\"\n",
    "    df_transcript = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "    return df_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a generator of turn-level ids, transcripts, and start and end times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_level_info_generator(\n",
    "        df_transcript: pd.DataFrame\n",
    ") -> Generator[Tuple[str, str], None, None]:\n",
    "    user_mask = (df_transcript[\"speaker\"] == \"user\")\n",
    "    \n",
    "    intro_mask = (df_transcript[\"topic\"] == \"intro\")\n",
    "    closing_mask = (df_transcript[\"topic\"] == \"closing\")\n",
    "    topic_mask = intro_mask | closing_mask\n",
    "\n",
    "    mask = user_mask & (~topic_mask)\n",
    "\n",
    "    df_transcript_masked = df_transcript[mask]\n",
    "    \n",
    "    for idx in df_transcript_masked.index:\n",
    "        transcript = df_transcript_masked.at[idx, \"transcript\"]\n",
    "\n",
    "        turn_id = str(idx).zfill(3)\n",
    "\n",
    "        yield turn_id, transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to load a forced alignment output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fa_output(user_id: str, turn_id: str) -> pd.DataFrame:\n",
    "    fa_output_path = FA_DIR / f\"{user_id}_{turn_id}.csv\"\n",
    "    df_fa = pd.read_csv(fa_output_path)\n",
    "\n",
    "    return df_fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to check whether a forced alignment output includes star tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_star_token_included(df_fa: pd.DataFrame) -> bool:\n",
    "    mask_star_token = (df_fa[\"word\"] == \"*\")\n",
    "    \n",
    "    return bool(mask_star_token.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to convert original transcripts to word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_fa_transcript(transcript: str) -> np.ndarray:\n",
    "    # 1. temporally change an inaudible tags to star tokens\n",
    "    fa_transcript = transcript.replace(\"<inaudible>\", \"*\")\n",
    "\n",
    "    # 2. remove punctuations\n",
    "    for punct in PUNCTUATIONS:\n",
    "       fa_transcript = fa_transcript.replace(punct, \"\") \n",
    "    fa_transcript = fa_transcript.replace(\"-\", \"\")\n",
    "\n",
    "    # 3. remove other tags\n",
    "    tag_pattern = r\"\\<.*?\\>\"\n",
    "    fa_transcript = re.sub(tag_pattern, \" \", fa_transcript) \n",
    "\n",
    "    # 4. recover inaudible tags from temporal star tokens\n",
    "    fa_transcript = fa_transcript.replace(\"*\", \"<inaudible>\")\n",
    "\n",
    "    # 5. lower transcript\n",
    "    fa_transcript = fa_transcript.lower()\n",
    "\n",
    "    # 6. remove extra pauses\n",
    "    while \"  \" in fa_transcript:\n",
    "        fa_transcript = fa_transcript.replace(\"  \", \" \")\n",
    "\n",
    "    if fa_transcript[0] == \" \":\n",
    "        fa_transcript = fa_transcript[1:]\n",
    "    if fa_transcript[-1] == \" \":\n",
    "        fa_transcript = fa_transcript[:-1]\n",
    "\n",
    "    return np.array(fa_transcript.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to check the number of words in a rev-format transcript and the corresponding forced alignment output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_same_length(word_list: np.ndarray, df_fa: pd.DataFrame) -> bool:\n",
    "    return len(word_list) == len(df_fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to fill star tokens in a forced alignment output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_startokens(word_list: np.ndarray, df_fa: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_fa_filled = df_fa.copy(deep=True)\n",
    "\n",
    "    mask_star = (df_fa[\"word\"] == \"*\").to_numpy()\n",
    "\n",
    "    df_fa_filled.loc[mask_star, \"word\"] = word_list[mask_star]\n",
    "\n",
    "    return df_fa_filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to save a forced alignment output in which star tokens are filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_fa_filled(df_fa_filled: pd.DataFrame, user_id: str, turn_id: str) -> None:\n",
    "    save_path = FA_DIR / f\"{user_id}_{turn_id}_filled.csv\"\n",
    "\n",
    "    df_fa_filled.to_csv(save_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Conduct Post-Process\n",
    "\n",
    "The following code block conducts post-process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for uid in user_id_generator():\n",
    "    df_transcript = load_transcript(uid)\n",
    "    \n",
    "    for tid, transcript in turn_level_info_generator(df_transcript):\n",
    "        if isinstance(transcript, float):\n",
    "            continue\n",
    "\n",
    "        df_fa = load_fa_output(uid, tid)\n",
    "\n",
    "        if not is_star_token_included(df_fa):\n",
    "            save_df_fa_filled(df_fa, uid, tid)\n",
    "            continue\n",
    "\n",
    "        word_list = convert_fa_transcript(transcript)\n",
    "        if is_same_length(word_list, df_fa):\n",
    "            df_fa_filled = fill_startokens(word_list, df_fa)\n",
    "            save_df_fa_filled(df_fa_filled, uid, tid)\n",
    "            continue\n",
    "\n",
    "        print(f\"[Error] {uid}_{tid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teai-incremental-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
