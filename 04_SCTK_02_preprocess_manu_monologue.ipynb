{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/13 (Mon) | SCTK\n",
    "\n",
    "# Preprocess of Manual Annotation of Monologic Corpus for SCTK Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook conducts the preprocess of manual annotation results for SCTK alignment.\n",
    "The goal of the preprocess is to generate csv files which have the following four columns:\n",
    "\n",
    "- start_time ... the start time of the row in sec\n",
    "- end_time ... the end time of the row in sec\n",
    "- type ... the type of the row (01_text, 02_pause, 03_disfl, 04_filler)\n",
    "- text ... the text of the row (word, \\<CI\\>, \\<CE\\>, \\<DISFLUENCU\\>, \\<FILLER\\>)\n",
    "\n",
    "The preprocess consists of the following procedures.\n",
    "\n",
    "1. Load a wav2vec forced alignment (FA) result csv file\n",
    "2. Load a textgrid corresponding to the csv file\n",
    "3. Extract texts from the textgrid\n",
    "4. Get 01_text type rows from the turn object\n",
    "5. Get 03_disfl type rows from the texts\n",
    "6. Get 02_pause type rows from the textgrid\n",
    "7. Get 03_filler type rows from the textgrid\n",
    "\n",
    "Before starting the preprocess, the following code block loads required packages and defnines global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List, Tuple, Generator\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from textgrids import TextGrid\n",
    "\n",
    "sys.path.append(\n",
    "    \"/home/matsuura/Development/app/feature_extraction_api/app/modules\"\n",
    ")\n",
    "\n",
    "from fluency import Turn\n",
    "\n",
    "DATA_DIR = Path(\"/home/matsuura/Development/app/feature_extraction_api/experiment/data\")\n",
    "\n",
    "TASK = [\"Arg_Oly\", \"Cartoon\", \"RtSwithoutRAA\", \"RtSwithRAA\"]\n",
    "\n",
    "PUNCTUATIONS = [\".\", \",\", \":\", \"?\", \"!\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Define Functions\n",
    "\n",
    "This section defines functions for the preprocess.\n",
    "The following code block defines a generator to yield a file path of FA csv and textgrid and a function to load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa_textgrid_path_generator(task: str) -> Generator[Tuple[Path, Path], None, None]:\n",
    "    load_dir = DATA_DIR / f\"{task}/01_Manual_TextGrid\"\n",
    "\n",
    "    for textgrid_path in load_dir.glob(\"*.TextGrid\"):\n",
    "        uid = textgrid_path.stem[:4]\n",
    "        \n",
    "        fa_filename = f\"{uid}_{task}_filled.csv\"\n",
    "        fa_csv_path = DATA_DIR / f\"{task}/06_FA_csv_Manu/{fa_filename}\"\n",
    "\n",
    "        yield fa_csv_path, textgrid_path\n",
    "\n",
    "def load_fa_and_textgrid(fa_path: Path, textgrid_path: Path) -> Tuple[pd.DataFrame, TextGrid]:\n",
    "    df_fa = pd.read_csv(fa_path)\n",
    "    textgrid = TextGrid(str(textgrid_path))\n",
    "\n",
    "    return df_fa, textgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to extract texts from textgrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_texts_from_textgrid(textgrid_path: Path) -> List[str]:\n",
    "    textgrid = TextGrid(str(textgrid_path))\n",
    "    transcript_tier = textgrid[\"Transcript\"]\n",
    "    \n",
    "    texts = []\n",
    "    for interval in transcript_tier:\n",
    "        text = interval.text\n",
    "        texts.append(text)\n",
    "\n",
    "    return texts\n",
    "\n",
    "def transform_texts(texts: List[str]) -> np.ndarray:\n",
    "    # 1. transform list 2 str\n",
    "    fa_transcript = \" \".join(texts)\n",
    "\n",
    "    # 2. remove punctuations\n",
    "    for punct in PUNCTUATIONS:\n",
    "        fa_transcript = fa_transcript.replace(punct, \" \")\n",
    "    fa_transcript = fa_transcript.replace(\"-\", \"\")\n",
    "    fa_transcript = fa_transcript.replace(\"é\", \"e\")\n",
    "\n",
    "    # 3. remove space before or after curly blacket\n",
    "    while \"{ \" in fa_transcript:\n",
    "        fa_transcript = fa_transcript.replace(\"{ \", \"{\")\n",
    "\n",
    "    while \" }\" in fa_transcript:\n",
    "        fa_transcript = fa_transcript.replace(\" }\", \"}\")\n",
    "\n",
    "    # 4. add space before and after curly blacket\n",
    "    fa_transcript = fa_transcript.replace(\"{\", \" {\")\n",
    "    fa_transcript = fa_transcript.replace(\"}\", \"} \")\n",
    "\n",
    "    # 5. lower transcript\n",
    "    fa_transcript = fa_transcript.lower()\n",
    "\n",
    "    # 6. remove extra pauses\n",
    "    while \"  \" in fa_transcript:\n",
    "        fa_transcript = fa_transcript.replace(\"  \", \" \")\n",
    "    \n",
    "    if fa_transcript[0] == \" \":\n",
    "        fa_transcript = fa_transcript[1:]\n",
    "    if fa_transcript[-1] == \" \":\n",
    "        fa_transcript = fa_transcript[:-1]\n",
    "\n",
    "    return np.array(fa_transcript.split(\" \"))\n",
    "\n",
    "def is_same_length(word_list: np.ndarray, df_fa: pd.DataFrame) -> bool:\n",
    "    return len(word_list) == len(df_fa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to generate text and disfl rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_disfl_rows(df_fa: pd.DataFrame, texts: np.ndarray) -> List[dict]:\n",
    "    is_in_curly_blanket = False\n",
    "    rows = []\n",
    "    for idx in df_fa.index:\n",
    "        start_tiem = df_fa.at[idx, \"start_time\"]\n",
    "        end_time = df_fa.at[idx, \"end_time\"]\n",
    "\n",
    "        text_row = {\n",
    "            \"start_time\": start_tiem,\n",
    "            \"end_time\": end_time,\n",
    "            \"type\": \"01_text\",\n",
    "            \"text\": df_fa.at[idx, \"word\"]\n",
    "        }\n",
    "        rows.append(text_row)\n",
    "\n",
    "        # TextGrid 上の書き起こしが {} に囲まれている場合\n",
    "        if is_in_curly_blanket:\n",
    "            disfl_row = {\n",
    "                \"start_time\": start_tiem,\n",
    "                \"end_time\": end_time,\n",
    "                \"type\": \"03_disfl\",\n",
    "                \"text\": \"<DISFLUENCY>\"\n",
    "            }\n",
    "            rows.append(disfl_row)\n",
    "\n",
    "            if \"}\" in texts[idx]: # その単語で {} が終了する場合\n",
    "                is_in_curly_blanket = False\n",
    "            \n",
    "            continue\n",
    "\n",
    "        # TextGrid 上の書き起こしの {} が開始する場合\n",
    "        if \"{\" in texts[idx]:\n",
    "            disfl_row = {\n",
    "                \"start_time\": start_tiem,\n",
    "                \"end_time\": end_time,\n",
    "                \"type\": \"03_disfl\",\n",
    "                \"text\": \"<DISFLUENCY>\"\n",
    "            }\n",
    "            rows.append(disfl_row)\n",
    "\n",
    "            if \"}\" not in texts[idx]: # {} に1単語しかない場合\n",
    "                is_in_curly_blanket = True\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to get pause and filler rows from a TextGrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rows_from_textgrid(textgrid: TextGrid) -> List[dict]:\n",
    "    pause_tier = textgrid[\"silences\"]\n",
    "    \n",
    "    rows = []\n",
    "    for interval in pause_tier:\n",
    "        pause_type = interval.text\n",
    "\n",
    "        if pause_type == \"\":\n",
    "            continue\n",
    "        \n",
    "        row = {\n",
    "            \"start_time\": interval.xmin,\n",
    "            \"end_time\": interval.xmax,\n",
    "            \"type\": \"02_pause\",\n",
    "            \"text\": f\"<{pause_type}>\"\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    filler_tier = textgrid[\"Filled\"]\n",
    "    for interval in filler_tier:\n",
    "        row = {\n",
    "            \"start_time\": interval.xpos,\n",
    "            \"end_time\": interval.xpos,\n",
    "            \"type\": \"04_filler\",\n",
    "            \"text\": \"<FILLER>\"\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Preprocess\n",
    "\n",
    "This section conducts the preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task in TASK:\n",
    "    save_dir = DATA_DIR / f\"{task}/10_SCTK_Inputs\"\n",
    "\n",
    "    for fa_path, textgrid_path in fa_textgrid_path_generator(task):\n",
    "        data = []\n",
    "        filename = fa_path.stem.removesuffix(\"_filled\")\n",
    "        save_path = save_dir / f\"{filename}_manu.csv\"\n",
    "\n",
    "        if save_path.exists():\n",
    "            continue\n",
    "\n",
    "        df_fa, textgrid = load_fa_and_textgrid(fa_path, textgrid_path)\n",
    "\n",
    "        texts = extract_texts_from_textgrid(textgrid_path)\n",
    "        texts = transform_texts(texts)\n",
    "\n",
    "        if not is_same_length(texts, df_fa):\n",
    "            raise Exception\n",
    "\n",
    "        data += extract_text_disfl_rows(df_fa, texts)\n",
    "        data += extract_rows_from_textgrid(textgrid)\n",
    "\n",
    "        df_annotation = pd.DataFrame.from_dict(data)\n",
    "        df_annotation = df_annotation.sort_values(\"start_time\").reset_index(drop=True)\n",
    "\n",
    "        df_annotation.to_csv(save_path, index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teai-incremental-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
