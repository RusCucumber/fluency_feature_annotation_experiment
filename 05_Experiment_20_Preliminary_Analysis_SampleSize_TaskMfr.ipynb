{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8/24 (Sat) | Experiment\n",
    "\n",
    "# Preliminary Analyses of Annoation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook conducts preliminary analyses.\n",
    "The goal of current analyses is to fill the following table.\n",
    "\n",
    "| Task | WER | N disfluency (Manual / Automatic) | N MCP (Manual / Automatic) | N ECP (Manual / Automatic) |\n",
    "| - | - | - | - | - |\n",
    "| Arg_Oly |  |  |  |  |\n",
    "| Cartoon |  |  |  |  |\n",
    "| RtSwithoutRAA |  |  |  |  |\n",
    "| RtSwithRAA |  |  |  |  |\n",
    "| Monologue |  |  |  |  |\n",
    "| WoZ_Interview |  |  |  |  |\n",
    "| ALL |  |  |  |  |\n",
    "\n",
    "Before starting the analyses, the following code block loads required packages and define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict, Generator, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.mfr import logit_2_rating\n",
    "\n",
    "DATA_DIR = Path(\"/home/matsuura/Development/app/feature_extraction_api/experiment/data\")\n",
    "\n",
    "MONOLOGUE_TASK = [\"Arg_Oly\", \"Cartoon\", \"RtSwithoutRAA\", \"RtSwithRAA\"]\n",
    "DIALOGUE_TASK = [\"WoZ_Interview\"]\n",
    "\n",
    "FILLER = {\"uh\", \"ah\", \"um\", \"mm\", \"hmm\", \"oh\", \"mm-hmm\", \"er\", \"mhm\", \"uh-huh\", \"er\", \"erm\", \"huh\", \"uhu\", \"mmhmm\", \"uhhuh\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Define Functions\n",
    "\n",
    "This section defines functions for the preliminary analyses.\n",
    "The following code block defines two functions; one generates csv file paths of manual and automatic annotation results; and another one loads them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_result_csv_path_generator(task: str, rating_filter: Optional[List[int]] =None) -> Generator[Tuple[Path, Path], None, None]:\n",
    "    load_dir = DATA_DIR / f\"{task}/10_SCTK_Inputs\"\n",
    "\n",
    "    if rating_filter is None:\n",
    "        for manu_csv_path in load_dir.glob(\"*_manu.csv\"):\n",
    "            filename = manu_csv_path.stem.removesuffix(\"_manu\")\n",
    "            auto_csv_path = load_dir / f\"{filename}_auto.csv\"\n",
    "\n",
    "            yield manu_csv_path, auto_csv_path\n",
    "    else:\n",
    "        pf_path = DATA_DIR / f\"{task}/12_PF_Rating/pf_rating.csv\"\n",
    "        df_pf = pd.read_csv(pf_path)\n",
    "        uid_list = df_pf[\"uid\"].to_numpy()\n",
    "\n",
    "        logit_path = pf_path.parent / \"logit_all.csv\"\n",
    "        threshold_path = logit_path.parent / \"threshold_all.csv\"\n",
    "        if task == \"WoZ_Interview\":\n",
    "            logit_path = pf_path.parent / \"logit.csv\"\n",
    "            threshold_path = logit_path.parent / \"threshold.csv\"\n",
    "        \n",
    "        df_logit = pd.read_csv(logit_path, index_col=0)\n",
    "        rating_list = logit_2_rating(df_logit[\"theta\"], threshold_path)\n",
    "\n",
    "        mask = np.full(rating_list.shape, False, dtype=bool)\n",
    "        for rating in rating_filter:\n",
    "            mask = mask | (rating_list == rating)\n",
    "        \n",
    "        uid_list = uid_list[mask]\n",
    "\n",
    "        for uid in uid_list:\n",
    "            if task == \"WoZ_Interview\":\n",
    "                uid = str(int(uid)).zfill(3)\n",
    "\n",
    "            filename_pattern = f\"{uid}*_manu.csv\"\n",
    "            for manu_csv_path in load_dir.glob(filename_pattern):\n",
    "                filename = manu_csv_path.stem.removesuffix(\"_manu\")\n",
    "                auto_csv_path = load_dir / f\"{filename}_auto.csv\"\n",
    "\n",
    "                yield manu_csv_path, auto_csv_path\n",
    "\n",
    "def load_dataset(\n",
    "        rating_filter_monologue: Optional[List[int]] =None,\n",
    "        rating_filter_dialogue: Optional[List[int]] =None,\n",
    ") -> Dict[str, Dict[str, List[Dict[str, pd.DataFrame]]]]:\n",
    "    dataset = {\n",
    "        \"monologue\": {},\n",
    "        \"dialogue\": {}\n",
    "    }\n",
    "    \n",
    "    for monologue_task in MONOLOGUE_TASK:\n",
    "        dataset[\"monologue\"][monologue_task] = []\n",
    "        \n",
    "        for manu_csv_path, auto_csv_path in annotation_result_csv_path_generator(monologue_task, rating_filter=rating_filter_monologue):\n",
    "            df_manu = pd.read_csv(manu_csv_path)\n",
    "            df_auto = pd.DataFrame([], columns=[\"text\"])\n",
    "            if auto_csv_path.exists():\n",
    "                df_auto = pd.read_csv(auto_csv_path)\n",
    "\n",
    "            dataset[\"monologue\"][monologue_task].append({\n",
    "                \"manual\": df_manu,\n",
    "                \"automatic\": df_auto\n",
    "            })\n",
    "\n",
    "    for dialogue_task in DIALOGUE_TASK:\n",
    "        dataset[\"dialogue\"][dialogue_task] = []\n",
    "\n",
    "        for manu_csv_path, auto_csv_path in annotation_result_csv_path_generator(dialogue_task, rating_filter=rating_filter_dialogue):\n",
    "            df_manu = pd.read_csv(manu_csv_path, na_values=[\"\", \" \"], keep_default_na=False)\n",
    "            df_auto = pd.DataFrame([], columns=[\"text\"])\n",
    "            if auto_csv_path.exists():\n",
    "                df_auto = pd.read_csv(auto_csv_path, na_values=[\"\", \" \"], keep_default_na=False)\n",
    "\n",
    "            dataset[\"dialogue\"][dialogue_task].append({\n",
    "                \"manual\": df_manu,\n",
    "                \"automatic\": df_auto\n",
    "            })\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to count the number of tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_sample_size(annotation_results: List[Dict[str, pd.DataFrame]]) -> List[int]:    \n",
    "    n_tag_manu = []\n",
    "\n",
    "    for annotation_result in annotation_results:\n",
    "        df_manu = annotation_result[\"manual\"]\n",
    "\n",
    "        mask_tag_manu = ~(df_manu[\"text\"].str.contains(\"<\"))\n",
    "\n",
    "        n_tag_manu.append(mask_tag_manu.sum())\n",
    "\n",
    "    return n_tag_manu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Preliminary Analyses\n",
    "\n",
    "This section conducts the preliminary analyses.\n",
    "The following code block loads entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.240063</td>\n",
       "      <td>01_text</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300063</td>\n",
       "      <td>0.660125</td>\n",
       "      <td>01_text</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.820187</td>\n",
       "      <td>1.040250</td>\n",
       "      <td>01_text</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.260312</td>\n",
       "      <td>2.080563</td>\n",
       "      <td>01_text</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.132178</td>\n",
       "      <td>2.764380</td>\n",
       "      <td>02_pause</td>\n",
       "      <td>&lt;CE&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  end_time      type       text\n",
       "0    0.220000  0.240063   01_text          i\n",
       "1    0.300063  0.660125   01_text      agree\n",
       "2    0.820187  1.040250   01_text       this\n",
       "3    1.260312  2.080563   01_text  statement\n",
       "4    2.132178  2.764380  02_pause       <CE>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"monologue\"][\"Arg_Oly\"][0][\"manual\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.240063</td>\n",
       "      <td>01_text</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.300063</td>\n",
       "      <td>0.660125</td>\n",
       "      <td>01_text</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.820187</td>\n",
       "      <td>1.040250</td>\n",
       "      <td>01_text</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.260312</td>\n",
       "      <td>2.080563</td>\n",
       "      <td>01_text</td>\n",
       "      <td>statement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.080563</td>\n",
       "      <td>2.740750</td>\n",
       "      <td>02_pause</td>\n",
       "      <td>&lt;CI&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   start_time  end_time      type       text\n",
       "0    0.220000  0.240063   01_text          i\n",
       "1    0.300063  0.660125   01_text      agree\n",
       "2    0.820187  1.040250   01_text       this\n",
       "3    1.260312  2.080563   01_text  statement\n",
       "4    2.080563  2.740750  02_pause       <CI>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"monologue\"][\"Arg_Oly\"][0][\"automatic\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Count Words (Sample Size)\n",
    "\n",
    "The following code block counts the number of disfluency words in monologue tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Manual] N_sample_size of Arg_Oly = 14623\n",
      "[Manual] N_sample_size of Cartoon = 18818\n",
      "[Manual] N_sample_size of RtSwithoutRAA = 19228\n",
      "[Manual] N_sample_size of RtSwithRAA = 19251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Manual] N_sample_size of RtSwithRAA = 38757\n"
     ]
    }
   ],
   "source": [
    "for monologue_task in MONOLOGUE_TASK:\n",
    "    annotation_results = dataset[\"monologue\"][monologue_task]\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")\n",
    "\n",
    "for dialogue_task in DIALOGUE_TASK:\n",
    "    annotation_results = dataset[\"dialogue\"][dialogue_task]\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginner_dataset = load_dataset(rating_filter_monologue=[0, 1, 2], rating_filter_dialogue=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block counts disfluency tags in beginners' speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Manual] N_sample_size of Arg_Oly = 3109\n",
      "[Manual] N_sample_size of Cartoon = 3891\n",
      "[Manual] N_sample_size of RtSwithoutRAA = 4210\n",
      "[Manual] N_sample_size of RtSwithRAA = 3930\n",
      "[Manual] N_sample_size of RtSwithRAA = 7301\n"
     ]
    }
   ],
   "source": [
    "for monologue_task in MONOLOGUE_TASK:\n",
    "    annotation_results = beginner_dataset[\"monologue\"][monologue_task]\n",
    "\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")\n",
    "\n",
    "for dialogue_task in DIALOGUE_TASK:\n",
    "    annotation_results = beginner_dataset[\"dialogue\"][dialogue_task]\n",
    "\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Intemediate\n",
    "\n",
    "The following code block loads intermediate group's speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intemediate_dataset = load_dataset(rating_filter_monologue=[3, 4, 5], rating_filter_dialogue=[2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block counts disfluency tags in intemediate learners' speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Manual] N_sample_size of Arg_Oly = 7066\n",
      "[Manual] N_sample_size of Cartoon = 9492\n",
      "[Manual] N_sample_size of RtSwithoutRAA = 9223\n",
      "[Manual] N_sample_size of RtSwithRAA = 9622\n",
      "[Manual] N_sample_size of RtSwithRAA = 24792\n"
     ]
    }
   ],
   "source": [
    "for monologue_task in MONOLOGUE_TASK:\n",
    "    annotation_results = intemediate_dataset[\"monologue\"][monologue_task]\n",
    "\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")\n",
    "\n",
    "for dialogue_task in DIALOGUE_TASK:\n",
    "    annotation_results = intemediate_dataset[\"dialogue\"][dialogue_task]\n",
    "\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Advanced \n",
    "\n",
    "The following code block loads advanced learners' speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_dataset = load_dataset(rating_filter_monologue=[6, 7, 8], rating_filter_dialogue=[4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block counts disfluency tags in advanced learners' speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Manual] N_sample_size of Arg_Oly = 4448\n",
      "[Manual] N_sample_size of Cartoon = 5435\n",
      "[Manual] N_sample_size of RtSwithoutRAA = 5795\n",
      "[Manual] N_sample_size of RtSwithRAA = 5699\n",
      "[Manual] N_sample_size of RtSwithRAA = 6664\n"
     ]
    }
   ],
   "source": [
    "for monologue_task in MONOLOGUE_TASK:\n",
    "    annotation_results = advanced_dataset[\"monologue\"][monologue_task]\n",
    "\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")\n",
    "\n",
    "for dialogue_task in DIALOGUE_TASK:\n",
    "    annotation_results = advanced_dataset[\"dialogue\"][dialogue_task]\n",
    "\n",
    "    n_tags_manu = count_sample_size(annotation_results)\n",
    "    print(f\"[Manual] N_sample_size of {monologue_task} = {sum(n_tags_manu)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teai-incremental-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
