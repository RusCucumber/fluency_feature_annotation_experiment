{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/12 (Sun) | UF Measures\n",
    "\n",
    "# Shorten the Automatic Annotation Results for UF Measure Claculation in Monologic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook shorten the automated annotation results to calculate UF measures of monologic speech corpus.\n",
    "Since in monologic data fluency was judged based on first 1 minute of speech, I shorten the automatic annotation results.\n",
    "Procedures consist of the following stages.\n",
    "\n",
    "1. Load a pkl path of turn object\n",
    "2. Load a short transcript corresponding to the turn object\n",
    "3. Get end time of the transcript\n",
    "4. Shorten the turn object\n",
    "5. Generate a TextGrid from the shorten turn\n",
    "6. Save Turn and TextGrid\n",
    "\n",
    "Before starting the procedures, the following code block loads required packages and defines global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Generator\n",
    "import sys, json, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from textgrids import TextGrid\n",
    "\n",
    "sys.path.append(\n",
    "    \"/home/matsuura/Development/app/feature_extraction_api/app/modules\"\n",
    ")\n",
    "\n",
    "from fluency import shorten_turn, Annotator, Turn\n",
    "from fluency.pipeline.utils.pause_location import PauseLocation\n",
    "\n",
    "DATA_DIR = Path(\"/home/matsuura/Development/app/feature_extraction_api/experiment/data\")\n",
    "\n",
    "TASK = [\"Arg_Oly\", \"Cartoon\", \"RtSwithoutRAA\", \"RtSwithRAA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Define Functions\n",
    "\n",
    "This section defines functions to shorten the automatic annotatio results.\n",
    "The following code block defines a generator to yield a pkl path of Turn object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_path_generator(task: str) -> Generator[Path, None, None]:\n",
    "    load_dir = DATA_DIR / f\"{task}/08_Auto_Annotation\"\n",
    "\n",
    "    for turn_path in load_dir.glob(f\"*_{task}_long_bert.pkl\"):\n",
    "        yield turn_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to load Turn object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_turn(turn_path: Path) -> Turn:\n",
    "    with open(turn_path, \"rb\") as f:\n",
    "        turn = pkl.load(f)\n",
    "\n",
    "    return turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to load a shor transcript as pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_short_transcript(turn_path: Path, task: str) -> pd.DataFrame:\n",
    "    filename = turn_path.stem.removesuffix(\"_long_bert\")\n",
    "    load_path = DATA_DIR / f\"{task}/02_Rev_Transcript/{filename}.wav.csv\"\n",
    "\n",
    "    df_short_transcript = pd.read_csv(load_path, index_col=0)\n",
    "\n",
    "    return df_short_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to get the end time of the loaded short transcript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_end_time(df_short_transcript: pd.DataFrame) -> float:\n",
    "    mask_text = (df_short_transcript[\"type\"] == \"text\")\n",
    "    df_short_transcript_masked = df_short_transcript[mask_text]\n",
    "\n",
    "    end_time = df_short_transcript_masked.iloc[-1][\"end_time\"]\n",
    "\n",
    "    return end_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to shorten corresponding TextGrid file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pauses(turn: Turn) -> List[dict]:\n",
    "    pauses = []\n",
    "    prev_clause_end = turn.start_time\n",
    "    for clause in turn.clauses:\n",
    "        if clause.start_time - prev_clause_end >= 0.25:\n",
    "            p = {\n",
    "                \"location\": PauseLocation.CLAUSE_EXTERNAL,\n",
    "                \"start_time\": prev_clause_end,\n",
    "                \"end_time\": clause.start_time\n",
    "            }\n",
    "            pauses.append(p)\n",
    "\n",
    "        prev_word_end = clause.start_time\n",
    "        for word in clause.words:\n",
    "            if word.start_time - prev_word_end >= 0.25:\n",
    "                p = {\n",
    "                    \"location\": PauseLocation.CLAUSE_INTERNAL,\n",
    "                    \"start_time\": prev_word_end,\n",
    "                    \"end_time\": word.start_time\n",
    "                }\n",
    "                pauses.append(p)\n",
    "\n",
    "            prev_word_end = word.end_time\n",
    "\n",
    "        prev_clause_end = clause.end_time\n",
    "\n",
    "    return pauses\n",
    "\n",
    "\n",
    "def shorten_textgrid(short_turn: Turn, annotator: Annotator, textgrid_path: Path) -> TextGrid:\n",
    "    pauses = find_pauses(short_turn)\n",
    "    short_textgrid = annotator.to_textgrid(short_turn, pauses, save_path=textgrid_path)\n",
    "\n",
    "    return short_textgrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Shorten Turn\n",
    "\n",
    "This section shorten turn objects and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotator = Annotator(process=[])\n",
    "\n",
    "for task in TASK:\n",
    "    for turn_path in turn_path_generator(task):\n",
    "        turn = load_turn(turn_path)\n",
    "        df_short_transcript = load_short_transcript(turn_path, task)\n",
    "        end_time = get_end_time(df_short_transcript)\n",
    "\n",
    "        filename = turn_path.stem.removesuffix(\"_long_bert\")\n",
    "        short_turn_path = turn_path.parent / f\"{filename}_bert.pkl\"\n",
    "        short_textgrid_path = turn_path.parent / f\"{filename}_bert.TextGrid\"\n",
    "\n",
    "        short_turn = shorten_turn(turn, end_time)\n",
    "        short_textgrid = shorten_textgrid(short_turn, annotator, short_textgrid_path)\n",
    "\n",
    "        with open(short_turn_path, \"wb\") as f:\n",
    "            pkl.dump(short_turn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teai-incremental-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
