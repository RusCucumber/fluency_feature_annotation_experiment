{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5/11 (Sat) | UF Measures\n",
    "\n",
    "# Automatic Annotation of Temporal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "This notebook annotated temporal features using the proposed pipeline.\n",
    "Before starting the automatic annotation, the following code block loads required packages and defines global variabls.\n",
    "Note that instead of RoBERTa based disfluency detector, this notebook utilized the conventional BERT based detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Generator\n",
    "import sys, json, traceback\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "from textgrids import TextGrid\n",
    "from rev_ai import Transcript\n",
    "\n",
    "sys.path.append( # TODO: PYTHONPATH 環境変数によるモジュール読み込みを修正する\n",
    "    \"/home/matsuura/Development/app/feature_extraction_api\"\n",
    ")\n",
    "sys.path.append(\n",
    "    \"/home/matsuura/Development/app/feature_extraction_api/app/modules\"\n",
    ")\n",
    "\n",
    "from app.utils.rev_utils import transcript_2_df, FILLER\n",
    "from fluency import Turn, DisfluencyEnum, Annotator\n",
    "from fluency.pipeline.utils.pause_location import PauseLocation\n",
    "\n",
    "DATA_DIR = Path(\"/home/matsuura/Development/app/feature_extraction_api/experiment/data\")\n",
    "\n",
    "TASK = [\"WoZ_Interview\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Define Functions\n",
    "\n",
    "This section defines functions to conduct automatic annotation.\n",
    "The following code block defines a generator to yield rev transcript json path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_path_generator(task: str) -> Generator[Path, None, None]:\n",
    "    load_dir = DATA_DIR / f\"{task}/07_Rev_Json\"\n",
    "\n",
    "    for json_path in load_dir.glob(\"*.json\"):\n",
    "        yield json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function for the preprocess of rev transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_turn(\n",
    "        rev_transcript: Transcript\n",
    ") -> Tuple[pd.DataFrame, List[int], List[int]]:\n",
    "    df_rev = transcript_2_df(rev_transcript)\n",
    "\n",
    "    period_locations = [] # ピリオドが挿入される直前の word id のリスト\n",
    "    filler_locations = [] # フィラーの word id のリスト\n",
    "    idx = -1\n",
    "    for i in df_rev.index:\n",
    "        w = df_rev.at[i, \"text\"]\n",
    "        t = df_rev.at[i, \"type\"]\n",
    "\n",
    "        if t == \"text\":\n",
    "            idx += 1\n",
    "            if w.lower() in FILLER:\n",
    "                filler_locations.append(idx)\n",
    "            if \" \" in w:\n",
    "                w = w.replace(\" \", \"_\")\n",
    "                df_rev.at[i, \"text\"] = w\n",
    "        else:\n",
    "            if w == \".\":\n",
    "                period_locations.append(idx - len(filler_locations))\n",
    "\n",
    "    df_text = df_rev[df_rev[\"type\"] != \"punct\"].reset_index()\n",
    "    df_text[\"text\"] = df_text[\"text\"].str.lower()\n",
    "\n",
    "    if -1 in period_locations:\n",
    "        period_locations.remove(-1)\n",
    "        \n",
    "    period_locations = sorted(set(period_locations))\n",
    "\n",
    "    return df_text, period_locations, filler_locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a fuction to convert from pandas' DataFrame to Turn object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_2_turn(\n",
    "        df_rev: pd.DataFrame, \n",
    "        period_locations: List[int], \n",
    "        filler_locations: List[int]\n",
    ") -> Turn:\n",
    "    turn = Turn.from_DataFrame(df_rev, 0, word_col=\"text\")\n",
    "\n",
    "    disfluency_list = [DisfluencyEnum.FILLER for _ in filler_locations]\n",
    "    turn.clauses[0].annotate_disfluency(filler_locations, disfluency_list)\n",
    "    turn.reset_words()\n",
    "\n",
    "    if len(period_locations) != 0:\n",
    "        turn.separate_clause(0, period_locations[:-1])\n",
    "\n",
    "    return turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines to find pauses from FA timestamp information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pauses(turn: Turn) -> List[dict]:\n",
    "    pauses = []\n",
    "    prev_clause_end = turn.start_time\n",
    "    for clause in turn.clauses:\n",
    "        if clause.start_time - prev_clause_end >= 0.25:\n",
    "            p = {\n",
    "                \"location\": PauseLocation.CLAUSE_EXTERNAL,\n",
    "                \"start_time\": prev_clause_end,\n",
    "                \"end_time\": clause.start_time\n",
    "            }\n",
    "            pauses.append(p)\n",
    "\n",
    "        prev_word_end = clause.start_time\n",
    "        for wid, word in enumerate(clause.words):\n",
    "            if clause.idx == 0 and wid == 0: # 最初の節の最初の単語の場合\n",
    "                if len(clause) == 1: # 1単語のみからなる clause の場合，次の節へ\n",
    "                        continue\n",
    "                \n",
    "                if word.idx == -1 and word.disfluency.name == \"FILLER\":                \n",
    "                    prev_word_end = clause.words[wid + 1].end_time\n",
    "                    continue\n",
    "\n",
    "            if word.start_time - prev_word_end >= 0.25:\n",
    "                p = {\n",
    "                    \"location\": PauseLocation.CLAUSE_INTERNAL,\n",
    "                    \"start_time\": prev_word_end,\n",
    "                    \"end_time\": word.start_time\n",
    "                }\n",
    "                pauses.append(p)\n",
    "\n",
    "            prev_word_end = word.end_time\n",
    "\n",
    "        prev_clause_end = clause.end_time\n",
    "\n",
    "    return pauses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block defines a function to annotate temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate(\n",
    "        transcript: dict, \n",
    "        save_path: str, \n",
    "        annotator: Annotator\n",
    ") -> Tuple[Turn, TextGrid]:\n",
    "    df_transcript, period_locations, filler_locations = preprocess_for_turn(transcript)\n",
    "    turn = df_2_turn(df_transcript, period_locations, filler_locations)\n",
    "\n",
    "    turn.ignore_disfluency()\n",
    "    turn = annotator(turn=turn)\n",
    "    turn.show_disfluency()\n",
    "\n",
    "    pauses = find_pauses(turn)\n",
    "    grid = annotator.to_textgrid(turn, pauses, save_path=save_path)\n",
    "\n",
    "    return turn, grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Annotate Temporal Features\n",
    "\n",
    "This section annotates temporal features related to utterance fluency.\n",
    "The following code block constructs an annotator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model was selected!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-cased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "annotator = Annotator(process=[\"eos_detect\", \"pruning\", \"clause_detect\"], disfluency_detector=\"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block annotates temporal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 11 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 7 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 5 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 26 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 8 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 4 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 14 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 0 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 45 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n",
      "/home/matsuura/Development/app/feature_extraction_api/app/modules/fluency/common/turn.py:130: UserWarning: specified idx 60 is end word of clause\n",
      "  warn(f\"specified idx {idx} is end word of clause\")\n"
     ]
    }
   ],
   "source": [
    "for task in TASK:\n",
    "    save_dir = DATA_DIR / f\"{task}/08_Auto_Annotation\"\n",
    "\n",
    "    for json_path in json_path_generator(task):\n",
    "        with open(json_path, \"r\") as f:\n",
    "            transcript = Transcript.from_json(json.load(f))\n",
    "\n",
    "        turn_path = save_dir / f\"{json_path.stem}_bert.pkl\"    \n",
    "        textgrid_path = save_dir / f\"{json_path.stem}_bert.TextGrid\"\n",
    "\n",
    "        # if turn_path.exists() and textgrid_path.exists():\n",
    "        #     continue\n",
    "\n",
    "        try:\n",
    "            turn, textgrid = annotate(transcript, str(textgrid_path), annotator)\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error in {str(json_path)}:\\n{traceback.format_exc(e)}\")\n",
    "            continue\n",
    "\n",
    "        with open(turn_path, \"wb\") as f:\n",
    "            pkl.dump(turn, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teai-incremental-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
